{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "fb4HNZZq7LUm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wp6dXD27Hin"
      },
      "outputs": [],
      "source": [
        "!pip install imbalanced-learn\n",
        "!pip install keras-tuner\n",
        "import os\n",
        "import csv\n",
        "import warnings\n",
        "import IPython\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import sklearn.metrics as skm\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
        "from keras import layers, models, utils\n",
        "from keras.models import Sequential\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Activation, Flatten, Dense, Dropout, BatchNormalization\n",
        "from keras.preprocessing import image\n",
        "from scipy.stats import uniform, randint\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from tensorflow.keras import regularizers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "import librosa.display\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras_tuner import RandomSearch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Drive"
      ],
      "metadata": {
        "id": "QAYYV0Dg7N3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.simplefilter(\"ignore\")"
      ],
      "metadata": {
        "id": "jXTHU69q7Ov8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "79X4WWtU7Qbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting Dataframe"
      ],
      "metadata": {
        "id": "1SIM_UuQ7UQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define o caminho raiz onde o conjunto de dados está armazenado\n",
        "rootPath = \"/content/gdrive/MyDrive/GoogleColab/dataset_Autoral/dataset\"\n",
        "\n",
        "# Inicializa listas vazias para armazenar caminhos de arquivos e seus respectivos estados de humor\n",
        "paths = []\n",
        "moods = []\n",
        "\n",
        "# Itera sobre as pastas de humor no caminho raiz\n",
        "for mood in os.listdir(rootPath):\n",
        "    # Itera sobre os arquivos dentro de cada pasta de humor\n",
        "    for file in os.listdir(rootPath + \"/\" + mood):\n",
        "        # Constrói o caminho completo do arquivo e o adiciona à lista de caminhos\n",
        "        paths.append(rootPath + \"/\" + mood + \"/\" + file)\n",
        "        # Adiciona o humor atual à lista de estados de humor\n",
        "        moods.append(mood)\n",
        "\n",
        "# Cria um DataFrame do Pandas com as colunas \"filePath\" e \"mood\"\n",
        "data_CNN_Aut = pd.DataFrame(columns=[\"filePath\", \"mood\"])\n",
        "\n",
        "# Atribui as listas de caminhos de arquivo e estados de humor às respectivas colunas do DataFrame\n",
        "data_CNN_Aut[\"filePath\"] = paths\n",
        "data_CNN_Aut[\"mood\"] = moods"
      ],
      "metadata": {
        "id": "p0LeHzm47WLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "gVooLAb67X9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define funções para criar espectrogramas e carregar imagens\n",
        "def create_spectrogram(audio_file, image_file):\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1)\n",
        "    y, sr = librosa.load(audio_file)\n",
        "    ms = librosa.feature.melspectrogram(y=y, sr=sr)\n",
        "    log_ms = librosa.power_to_db(ms, ref=np.max)\n",
        "    librosa.display.specshow(log_ms, sr=sr)\n",
        "    fig.savefig(image_file)\n",
        "    plt.close(fig)\n",
        "\n",
        "def create_pngs_from_wavs(input_path, output_path):\n",
        "    if not os.path.exists(output_path):\n",
        "        os.makedirs(output_path)\n",
        "    for file in os.listdir(input_path):\n",
        "        if file.endswith('.wav'):\n",
        "            input_file = os.path.join(input_path, file)\n",
        "            output_file = os.path.join(output_path, file.replace('.wav', '.png'))\n",
        "            create_spectrogram(input_file, output_file)\n",
        "\n",
        "# Gerar espectrogramas para cada categoria de emoção\n",
        "emotions = [\"aggressive\", \"dramatic\", \"happy\", \"romantic\", \"sad\"]\n",
        "for emotion in emotions:\n",
        "    create_pngs_from_wavs(f\"/content/gdrive/MyDrive/GoogleColab/dataset_Autoral/dataset/{emotion}\", f\"/content/gdrive/MyDrive/GoogleColab/dataset_Autoral/Spectograms/{emotion}\")\n",
        "\n",
        "# Função para carregar imagens a partir de um diretório\n",
        "def load_images_from_path(path, label):\n",
        "    images_CNN_Aut = []\n",
        "    labels_CNN_Aut = []\n",
        "    for file in os.listdir(path):\n",
        "        if file.endswith('.png'):\n",
        "            img = image.load_img(os.path.join(path, file), target_size=(224, 224))\n",
        "            img_array = image.img_to_array(img)\n",
        "            images_CNN_Aut.append(img_array)\n",
        "            labels_CNN_Aut.append(label)\n",
        "    return images_CNN_Aut, labels_CNN_Aut\n",
        "\n",
        "# Carregar as imagens e os rótulos\n",
        "x_CNN_Aut = []\n",
        "y_CNN_Aut = []\n",
        "\n",
        "for emotion in emotions:\n",
        "    images, labels = load_images_from_path(f\"/content/gdrive/MyDrive/GoogleColab/dataset_Autoral/Spectograms/{emotion}\", emotion)\n",
        "    x_CNN_Aut.extend(images)\n",
        "    y_CNN_Aut.extend(labels)\n",
        "\n",
        "# Converter listas para arrays numpy\n",
        "x_CNN_Aut = np.array(x_CNN_Aut)\n",
        "y_CNN_Aut = np.array(y_CNN_Aut)\n",
        "\n",
        "# Codificar rótulos\n",
        "label_encoder = LabelEncoder()\n",
        "y_CNN_Aut_encoded = label_encoder.fit_transform(y_CNN_Aut)\n",
        "\n",
        "# Divisão do dataset em treino e teste\n",
        "x_train_CNN_Aut, x_test_CNN_Aut, y_train_CNN_Aut, y_test_CNN_Aut = train_test_split(x_CNN_Aut, y_CNN_Aut_encoded, stratify=y_CNN_Aut_encoded, test_size=0.2, random_state=0)\n",
        "\n",
        "# Normalização\n",
        "x_train_norm_CNN_Aut = x_train_CNN_Aut / 255.0\n",
        "x_test_norm_CNN_Aut = x_test_CNN_Aut / 255.0\n",
        "\n",
        "# Codificação categórica\n",
        "y_train_encoded_CNN_Aut = to_categorical(y_train_CNN_Aut)\n",
        "y_test_encoded_CNN_Aut = to_categorical(y_test_CNN_Aut)\n",
        "\n",
        "# Função para criar o modelo com hiperparâmetros variáveis\n",
        "def build_model(hp):\n",
        "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    x = base_model.output\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(units=hp.Int('units', min_value=128, max_value=1024, step=128), activation='relu')(x)\n",
        "    x = Dropout(rate=hp.Float('dropout', min_value=0.2, max_value=0.5, step=0.1))(x)\n",
        "    output = Dense(len(emotions), activation='softmax')(x)\n",
        "\n",
        "    model_CNN_Aut = Model(inputs=base_model.input, outputs=output)\n",
        "    model_CNN_Aut.compile(optimizer=Adam(learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model_CNN_Aut\n",
        "\n",
        "# Configurar a busca de hiperparâmetros\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=10,\n",
        "    executions_per_trial=2,\n",
        "    directory='hyperparam_tuning',\n",
        "    project_name='emotion_prediction'\n",
        ")\n",
        "\n",
        "# Executar a busca de hiperparâmetros\n",
        "tuner.search(x_train_norm_CNN_Aut, y_train_encoded_CNN_Aut, epochs=30, validation_data=(x_test_norm_CNN_Aut, y_test_encoded_CNN_Aut))\n",
        "\n",
        "# Resumo dos melhores hiperparâmetros\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(f\"Melhores hiperparâmetros: {best_hps.values}\")\n",
        "\n",
        "# Treinar o modelo com os melhores hiperparâmetros\n",
        "model_CNN_Aut = tuner.hypermodel.build(best_hps)\n",
        "history = model_CNN_Aut.fit(x_train_norm_CNN_Aut, y_train_encoded_CNN_Aut, epochs=30, validation_data=(x_test_norm_CNN_Aut, y_test_encoded_CNN_Aut))"
      ],
      "metadata": {
        "id": "t2GF9dFG7bk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Acuracy History"
      ],
      "metadata": {
        "id": "tWa7sEgC7tsS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, '-', label='Training Accuracy')\n",
        "plt.plot(epochs, val_acc, ':', label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.plot()"
      ],
      "metadata": {
        "id": "TEvLQMPC7xAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Results"
      ],
      "metadata": {
        "id": "bqT7BMAq7xfj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelResults:\n",
        "    def __init__(self, model, class_labels):\n",
        "        self.model = model\n",
        "        self.class_labels = class_labels\n",
        "\n",
        "    def predict(self, x_test):\n",
        "        y_predicted = self.model.predict(x_test)\n",
        "        return y_predicted\n",
        "\n",
        "    def plot_confusion_matrix(self, y_true, y_pred):\n",
        "        mat = confusion_matrix(y_true, y_pred)\n",
        "        sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',\n",
        "                    xticklabels=self.class_labels,\n",
        "                    yticklabels=self.class_labels)\n",
        "        plt.xlabel('Predicted label')\n",
        "        plt.ylabel('Actual label')\n",
        "        plt.show()\n",
        "\n",
        "    def classification_report(self, y_true, y_pred):\n",
        "        report = classification_report(y_true, y_pred, target_names=self.class_labels, digits=3)\n",
        "        print(report)\n",
        "\n",
        "    def evaluate(self, x_test, y_true):\n",
        "        y_predicted = self.predict(x_test)\n",
        "        y_pred_classes = np.argmax(y_predicted, axis=1)\n",
        "        y_true_classes = np.argmax(y_true, axis=1)\n",
        "        self.plot_confusion_matrix(y_true_classes, y_pred_classes)\n",
        "        self.classification_report(y_true_classes, y_pred_classes)\n",
        "\n",
        "# Supondo que você tenha treinado e salvo o modelo 'best_model'\n",
        "class_labels = ['aggressive', 'dramatic', 'happy', 'romantic', 'sad']\n",
        "model_results = ModelResults(model_CNN_Aut, class_labels)\n",
        "\n",
        "# Avaliar o modelo\n",
        "model_results.evaluate(x_test_norm_CNN_Aut, y_test_encoded_CNN_Aut)\n"
      ],
      "metadata": {
        "id": "VTmK3lG670sG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}